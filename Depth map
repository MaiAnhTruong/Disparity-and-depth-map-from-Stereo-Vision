{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19Wlb7DalIM_Wp3eDBqaOoa2hnuhuVeEm","authorship_tag":"ABX9TyM71+Y6cxGTmcDSfJM6Fqjo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import copy\n","from tqdm import tqdm"],"metadata":{"id":"nGaL2WWvL08h","executionInfo":{"status":"ok","timestamp":1744361528217,"user_tz":-420,"elapsed":9,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["# 1. Image Preprocessing\n","\n","In the process of building a 3D model from a 2D image, the image preprocessing step is very important. The goals of this step are:\n","\n","1. **Convert color image to grayscale image:**\n","Most feature detection algorithms do not need color information but focus on the structure and brightness information of the image. Therefore, converting the color image \\(I(x,y) = (R(x,y), G(x,y), B(x,y))\\) to grayscale image helps reduce the processing data and focus on the geometric structure of the image.\n","The usual conversion formula is:\n","$$ I_{\\text{gray}}(x,y) = 0.299\\cdot R(x,y) + 0.587\\cdot G(x,y) + 0.114\\cdot B(x,y) $$\n","This gives a grayscale image, where the values ​​determine the brightness of the pixel.\n","\n","2. **Blurring the image with a Gaussian filter:**\n","Most feature detection algorithms are sensitive to small noises. Therefore, before detecting features, we need to blur the image to remove unwanted noises. The Gaussian Blur function is defined as follows:\n","$$ G(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right) $$\n","Here, the kernel has a size of \\(15 \\times 15\\) (a relatively large size) and sigma is set to 0 (in OpenCV, if sigma = 0, the sigma value will be calculated automatically based on the kernel size).\n","\n","The image blurring process using convolution:\n","$$ I_{\\text{blur}}(x,y) = \\sum_{u,v} I_{\\text{gray}}(x-u, y-v) \\, G(u,v) $$\n","will blur small details and noise, making the feature points clearer."],"metadata":{"id":"_PMjPUJXjGi6"}},{"cell_type":"code","source":["def preprocessing(img):\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    processed_img = cv2.GaussianBlur(gray, (15,15), 0)\n","    return processed_img"],"metadata":{"id":"rgaNkUk2jFEp","executionInfo":{"status":"ok","timestamp":1744361528567,"user_tz":-420,"elapsed":33,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["# 2. Feature Detection and Matching\n","\n","### Objectives:\n","- **Keypoint Detection:** Find keypoints in an image using the SIFT algorithm – a scale and rotation invariant method.\n","- **Descriptor Calculation:** For each keypoint, the SIFT algorithm generates a descriptor vector containing the gradient information of the surrounding region.\n","- **Matching between images:** Use the BFMatcher (Brute-Force Matcher) to compare the descriptor vectors between two images.\n","- **Lowe's Ratio Test:** To eliminate uncertain matches, Lowe suggests comparing the distance between the best and second best match. A match is only accepted if:\n","\n","$$ d_1 < 0.7 \\times d_2 $$\n","This helps reduce false matches.\n","\n","### Detailed explanation:\n","\n","1. **Image Preprocessing:**\n","The input image is preprocessed (converted to grayscale and blurred) according to step 1.\n","\n","2. **SIFT Detection:**\n","SIFT searches for points with strong changes (extremes in Scale-Space) and calculates feature vectors for them.\n","\n","3. **Descriptor Matching:**\n","BFMatcher is used to compare the descriptors of image 1 and image 2. The result is a list of matching pairs (with the 2 best results).\n","\n","4. **Ratio Test:**\n","Each matching pair is tested against the above criteria. Pairs that do not satisfy the criteria are discarded."],"metadata":{"id":"a8zX5b5FmpJr"}},{"cell_type":"code","source":["def feature_detection(img1, img2):\n","    processed_img1, processed_img2 = preprocessing(img1), preprocessing(img2)\n","    sift = cv2.SIFT_create()\n","    kp1, desc1 = sift.detectAndCompute(processed_img1, None)\n","    kp2, desc2 = sift.detectAndCompute(processed_img2, None)\n","    bf = cv2.BFMatcher()\n","    matches = bf.knnMatch(desc1, desc2, k=2)\n","\n","    good = []\n","    for m, n in matches:\n","        if m.distance < 0.7 * n.distance:\n","            good.append([m])\n","\n","    left_pts  = np.float32([kp1[m[0].queryIdx].pt for m in good])\n","    right_pts = np.float32([kp2[m[0].trainIdx].pt for m in good])\n","    return left_pts, right_pts"],"metadata":{"id":"gJivAx9Esv8U","executionInfo":{"status":"ok","timestamp":1744361528854,"user_tz":-420,"elapsed":11,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["# 3. Fundamental Matrix Estimation\n","\n","### Objective:\n","- Determine the fundamental matrix \\( F \\) from the corresponding point pairs between the 2 images.\n","\n","- Matrix \\( F \\) implements the epipolar constraint:\n","\n","$$ \\mathbf{x'}^T \\, F \\, \\mathbf{x} = 0, $$\n","with \\(\\mathbf{x}=(x,y,1)^T\\) and \\(\\mathbf{x'}=(x', y', 1)^T\\).\n","\n","### Steps to follow:\n","1. **Build the system of equations:**\n","\n","For each pair of points \\((x,y) \\leftrightarrow (x',y')\\) we have:\n","\n","$$\n","x'x\\, f_{11} + x'y\\, f_{12} + x'\\, f_{13} + y'x\\, f_{21} + y'y\\, f_{22} + y'\\, f_{23} + x\\, f_{31} + y\\, f_{32} + f_{33} = 0\n","\n","$$\n","Each pair is arranged in a row of the matrix \\( A \\):\n","\n","$$\n","A_i = \\begin{bmatrix} x'x & x'y & x' & y'x & y'y & y' & x & y & 1 \\end{bmatrix}.\n","$$\n","\n","2. **Solve the linear system using SVD:**\n","\n","We solve the system \\( A\\mathbf{f} = 0 \\) through SVD analysis of \\( A \\):\n","\n","$$ A = U \\Sigma V^T, $$\n","vector \\( \\mathbf{f} \\) corresponding to the smallest singular value is reshaped into a matrix \\( F \\) (3×3).\n","\n","3. **Force \\( F \\) to have rank 2:**\n","\n","Since \\( F \\) needs to have rank 2, we perform SVD of \\( F \\):\n","\n","$$ F = U \\, \\text{diag}(s_1, s_2, s_3) \\, V^T, $$\n","then set \\( s_3 = 0 \\).\n","\n","Finally, we normalize \\( F \\) (e.g. divide by the element \\(F[-1, -1]\\) if it is not 0) and filter out very small values."],"metadata":{"id":"OU2PznuQsxM5"}},{"cell_type":"code","source":["def fundamental_matrix(left_pts, right_pts):\n","    A = []\n","    for i in range(8):\n","        src_x, src_y  = left_pts[i]\n","        dst_x, dst_y = right_pts[i]\n","        A_row = np.asarray([dst_x*src_x, dst_x*src_y, dst_x,\n","                            dst_y*src_x, dst_y*src_y, dst_y,\n","                            src_x, src_y, 1])\n","        A.append(A_row)\n","\n","    A = np.asarray(A)\n","    U, S, V = np.linalg.svd(A)\n","    F = np.reshape(V[-1, :], (3, 3))\n","    u, s, vt = np.linalg.svd(F)\n","    s = np.diag(s)\n","    s[2, 2] = 0\n","    F = u @ (s @ vt)\n","    if F[-1, -1] != 0:\n","        F = F / F[-1, -1]\n","    F = (abs(F) > 1e-3) * F\n","    return F"],"metadata":{"id":"GsY-psO5twvx","executionInfo":{"status":"ok","timestamp":1744361529171,"user_tz":-420,"elapsed":26,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":["# 4. RANSAC for Fundamental Matrix Estimation\n","\n","### Objective:\n","- Remove noise and outliers in the set of matched point pairs.\n","\n","- Repeat the \\(F\\) estimation process with random samples to find the most stable matrix.\n","\n","### Detailed procedure:\n","1. **Sampling:**\n","Randomly select 8 points from the set of matched points (requires at least 8 points).\n","\n","2. **Estimate \\( F \\):**\n","Calculate the \\( F \\) matrix from the selected sample using the function defined in step 3.\n","\n","3. **Inlier test:**\n","For all pairs of points, calculate the value:\n","\n","$$ c = \\mathbf{x'}^T\\, F\\,\\mathbf{x}. $$\n","If \\( |c| \\leq \\text{threshold} \\), consider that point as inlier.\n","\n","4. **Choose the optimal \\( F \\):**\n","The \\( F \\) with the highest number of inliers is selected."],"metadata":{"id":"vzNYk0EkuLeV"}},{"cell_type":"code","source":["def ransac(src_pts, dst_pts, iterations, threshold):\n","    number_of_points = len(src_pts)\n","    if number_of_points < 8:\n","        raise ValueError(\"Not enough feature points to calculate Fundamental matrix: need at least 8 points, but only {}\".format(number_of_points))\n","\n","    max_inlier_count = 0\n","    best_F = None\n","\n","    for i in range(iterations):\n","        inlier = 0\n","        random_indices = np.random.choice(number_of_points, size=8, replace=False)\n","        random_src_pts = src_pts[random_indices, :]\n","        random_dst_pts = dst_pts[random_indices, :]\n","\n","        F = fundamental_matrix(random_src_pts, random_dst_pts)\n","\n","        for j in range(number_of_points):\n","            a, b = dst_pts[j], src_pts[j]\n","            a = np.append(a, 1).reshape((3, 1))\n","            b = np.append(b, 1).reshape((3, 1))\n","            c = (a.T @ F) @ b\n","            if abs(c[0, 0]) <= threshold:\n","                inlier += 1\n","\n","        if inlier > max_inlier_count:\n","            max_inlier_count = inlier\n","            best_F = F\n","\n","    return best_F"],"metadata":{"id":"2YNMh00cumNB","executionInfo":{"status":"ok","timestamp":1744361529476,"user_tz":-420,"elapsed":15,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":["# 5. Calculate Essential Matrix\n","\n","### Objective:\n","- From the basis matrix \\(F\\) and the camera's intrinsic matrices \\(K_1\\) and \\(K_2\\), calculate the essential matrix \\(E\\).\n","- The essential matrix relates the images after being calibrated by the intrinsic matrices:\n","\n","$$ E = K_2^T\\, F\\, K_1. $$\n","\n","### Forcing the structure of \\(E\\):\n","- \\(E\\) needs to have two equal singular values ​​and the third value is 0, according to the geometrical properties.\n","- After calculating the SVD of \\(E\\), we reconstruct \\(E\\) with:\n","$$ E = U\\,\\text{diag}(1,1,0)\\,V^T. $$"],"metadata":{"id":"moXzKxlvuvKe"}},{"cell_type":"code","source":["def essential_matrix(F, K1, K2):\n","    E = (K2.T @ F) @ K1\n","    U, _, V = np.linalg.svd(E)\n","    S = [1, 1, 0]\n","    S = np.diag(S)\n","    E = U @ S @ V\n","    return E"],"metadata":{"id":"6Rnsvk-ou6Xu","executionInfo":{"status":"ok","timestamp":1744361529798,"user_tz":-420,"elapsed":9,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":["# 6. Decompose Essential Matrix\n","\n","### Goal:\n","- From the essential matrix \\(E\\), extract the possibilities of the rotation matrix \\(R\\) and the displacement vector \\(t\\) (or \\(C\\) – camera center).\n","\n","### Procedure:\n","1. **SVD decomposition of \\(E\\):**\n","$$ E = U\\,\\Sigma\\,V^T $$\n","\n","2. **Using the \\(W\\) matrix:**\n","Let \\( W \\) be defined as:\n","$$ W = \\begin{bmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$\n","From there, the possibilities of \\( R \\) are:\n","$$ R_1 = U\\,W\\,V^T \\quad \\text{and} \\quad R_2 = U\\,W^T\\,V^T. $$\n","\n","3. **Get Translation \\(t\\):**\n","The direction of \\(t\\) is determined by the 3rd column of \\(U\\):\n","$$ t \\propto \\pm U[:, 2]. $$\n","\n","4. **Create 4 possible configurations:**\n","Combine the 2 possibilities of \\(R\\) with the two signs of \\(t\\) to create 4 configurations. Then, use the \"positive depth\" test to choose the correct configuration."],"metadata":{"id":"aW2zl_Qev9E2"}},{"cell_type":"code","source":["def decompose_essential_matrix(E):\n","    W = np.array([[0, -1, 0],\n","                  [1,  0, 0],\n","                  [0,  0, 1]])\n","    U, S, V = np.linalg.svd(E)\n","\n","    R_set = []\n","    C_set = []\n","\n","    R_set.append(U @ W @ V)\n","    R_set.append(U @ W @ V)\n","    R_set.append(U @ W.T @ V)\n","    R_set.append(U @ W.T @ V)\n","    C_set.append(U[:, 2])\n","    C_set.append(-U[:, 2])\n","    C_set.append(U[:, 2])\n","    C_set.append(-U[:, 2])\n","\n","    for i in range(4):\n","        if np.linalg.det(R_set[i]) < 0:\n","            R_set[i] = -R_set[i]\n","            C_set[i] = -C_set[i]\n","\n","    return R_set, C_set"],"metadata":{"id":"tM0j3kH3wFjP","executionInfo":{"status":"ok","timestamp":1744361530132,"user_tz":-420,"elapsed":12,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":["# 7. Projection Matrix\n","\n","### Objective:\n","- Build a projection matrix \\( P \\) to convert 3D points \\( \\mathbf{X} \\) to 2D image coordinates \\( \\mathbf{x} \\).\n","- General projection formula:\n","\n","$$ \\mathbf{x} = P\\, \\mathbf{X} $$\n","with:\n","$$ P = K\\,[R\\,|\\, -R\\,C] $$\n","In which:\n","\n","- \\( K \\) is the intrinsic matrix.\n","\n","- \\( R \\) is the rotation matrix.\n","\n","- \\( C \\) is the camera center coordinate."],"metadata":{"id":"zgzey9sywcvH"}},{"cell_type":"code","source":["def projection_matrix(K, R, C):\n","    I  = np.identity(3)\n","    C = np.reshape(C, (3, 1))\n","    return K @ (R @ np.hstack((I, -C)))"],"metadata":{"id":"_lsFrYeB0E1c","executionInfo":{"status":"ok","timestamp":1744361530448,"user_tz":-420,"elapsed":15,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":["# 8. Triangulation: Calculate 3D Coordinates (Linear Triangulation)\n","\n","### Objective:\n","- From the corresponding pairs of points \\( \\mathbf{x} = (x,y,1)^T \\) and \\( \\mathbf{x'} = (x', y', 1)^T \\) together with projection matrices \\( P_1 \\) and \\( P_2 \\), we calculate the 3D coordinates \\( \\mathbf{X} \\).\n","\n","### Detailed mathematical process:\n","1. For each pair of points, we have two systems of equations:\n","\n","$$ \\mathbf{x} = P_1 \\, \\mathbf{X} \\quad \\text{và} \\quad \\mathbf{x'} = P_2 \\, \\mathbf{X}. $$\n","\n","2. From there, we construct a linear system based on each row of \\(P_1\\) and \\(P_2\\):\n","\n","- For example, if \\(P_1 = \\begin{bmatrix} p_1^T \\\\ p_2^T \\\\ p_3^T \\end{bmatrix}\\), we have:\n","\n","\\[\n","\\begin{aligned}\n","x (p_3^T \\, \\mathbf{X}) &= p_1^T \\, \\mathbf{X}, \\\\\n","y (p_3^T \\, \\mathbf{X}) &= p_2^T \\, \\mathbf{X}.\n","\\end{aligned}\n","\\]\n","\n","- Similarly for \\( P_2 \\).\n","\n","3. Combine the equations into a matrix \\( A \\) and solve the problem:\n","\n","$$ A\\,\\mathbf{X} = 0. $$\n","We use the SVD method to find the solution \\( \\mathbf{X} \\) in homogeneous form, then divide by the last component to convert to affine coordinates.\n"],"metadata":{"id":"i34Ltv-b2hVX"}},{"cell_type":"code","source":["def linear_triangulation(R_set, C_set, left_pts, right_pts, K1, K2):\n","    x3D_set = []\n","\n","    for i in range(len(R_set)):\n","        R1, R2 = np.identity(3), R_set[i]\n","        C1, C2 = np.zeros((3, 1)), C_set[i].reshape(3, 1)\n","\n","        P1 = projection_matrix(K1, R1, C1)\n","        P2 = projection_matrix(K2, R2, C2)\n","\n","        p1, p2, p3 = P1\n","        p1_, p2_, p3_ = P2\n","\n","        p1, p2, p3 =  p1.reshape(1, -1), p2.reshape(1, -1), p3.reshape(1, -1)\n","        p1_, p2_, p3_ =  p1_.reshape(1, -1), p2_.reshape(1, -1), p3_.reshape(1, -1)\n","\n","        x3D = []\n","        for left_pt, right_pt in zip(left_pts, right_pts):\n","            x, y = left_pt\n","            x_, y_ = right_pt\n","            A = np.vstack((y * p3 - p2,\n","                           p1 - x * p3,\n","                           y_ * p3_ - p2_,\n","                           p1_ - x_ * p3_))\n","            _, _, Vt = np.linalg.svd(A)\n","            X = Vt[-1]\n","            X = X / X[-1]\n","            x3D.append(X[:3])\n","\n","        x3D_set.append(x3D)\n","    return x3D_set"],"metadata":{"id":"mDptCdV62mgd","executionInfo":{"status":"ok","timestamp":1744361530796,"user_tz":-420,"elapsed":50,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":["# 9. Choose the Correct Camera Configuration (Disambiguation)\n","\n","### Objective:\n","- Of the 4 possible combinations of \\( R \\) and \\( t \\) from the Essential Matrix decomposition step, only the correct configuration allows the 3D points to have \"positive depth\" (i.e., in front of the camera).\n","\n","### Testing procedure:\n","1. For each configuration, perform triangulation and calculate the 3D points.\n","2. Check the \"positive depth\" condition by calculating:\n","$$ r_3^T (X - C) > 0 $$\n","where \\( r_3 \\) is the 3rd row of \\( R \\) – representing the camera's viewing direction.\n","3. The configuration with the largest number of points satisfying the above condition is selected as the main configuration."],"metadata":{"id":"qfOBKT7o3kbR"}},{"cell_type":"code","source":["def disambiguate_camera_pose(R_set, C_set, x3D_set):\n","    best_i = 0\n","    max_positive_depths = 0\n","\n","    for i in range(len(R_set)):\n","        n_positive_depths = 0\n","        R, C = R_set[i], C_set[i].reshape(-1, 1)\n","        r3 = R[2].reshape(1, -1)\n","        x3D = x3D_set[i]\n","\n","        for X in x3D:\n","            X = X.reshape(-1, 1)\n","            if (r3 @ (X - C)).item() > 0 and X[2].item() > 0:\n","                n_positive_depths += 1\n","\n","        if n_positive_depths > max_positive_depths:\n","            best_i = i\n","            max_positive_depths = n_positive_depths\n","\n","    R, C = R_set[best_i], C_set[best_i]\n","    return R, C"],"metadata":{"id":"vARfAs923qnq","executionInfo":{"status":"ok","timestamp":1744361531108,"user_tz":-420,"elapsed":43,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":["# 10. Calculate Epipolar Coordinates\n","\n","### Objective:\n","- From the equation of the epipolar line \\( ax + by + c = 0 \\), it is necessary to determine two boundary points to draw the line on the image.\n","\n","### Cases:\n","1. **When \\(b \\neq 0\\):**\n","\n","- With \\( x = 0 \\):\n","$$ y = -\\frac{c}{b}. $$\n","- With \\( x = \\text{width} \\):\n","$$ y = -\\frac{c + a\\cdot\\text{width}}{b}. $$\n","2. **When \\(b \\) is close to 0 (vertical line):**\n","\n","- Calculate:\n","$$ x = -\\frac{c}{a}, $$\n","- \\( y \\) runs from 0 to the height of the image."],"metadata":{"id":"kf1_j9Ao3vVi"}},{"cell_type":"code","source":["def epiline_coordinates(lines, img):\n","    lines = lines.reshape(-1, 3)\n","    c = img.shape[1]\n","    coordinates = []\n","\n","    for line in lines:\n","        if abs(line[1]) < 1e-6:\n","            if abs(line[0]) > 1e-6:\n","                x_const = -line[2] / line[0]\n","            else:\n","                x_const = 0\n","            x0, y0 = int(x_const), 0\n","            x1, y1 = int(x_const), img.shape[0]\n","        else:\n","            x0, y0 = 0, int(-line[2] / line[1])\n","            x1, y1 = c, int(-(line[2] + line[0] * c) / line[1])\n","        coordinates.append([[x0, y0], [x1, y1]])\n","\n","    return coordinates"],"metadata":{"id":"6Iu5S9LS4IFY","executionInfo":{"status":"ok","timestamp":1744361531407,"user_tz":-420,"elapsed":25,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":["# 11. Draw Epipolar Lines\n","\n","### Objective:\n","\n","- Based on the coordinates calculated from the `epiline_coordinates` function, this function will:\n","\n","1. Draw epipolar lines on the image.\n","\n","2. Mark keypoints with circles.\n","\n","- Use random colors for each pair to help distinguish lines and points."],"metadata":{"id":"lUZVyfOE4m-l"}},{"cell_type":"code","source":["def draw_epilines(l_epiline_coords, r_epiline_coords, left_pts, right_pts, img1, img2):\n","    img1_copy = copy.deepcopy(img1)\n","    img2_copy = copy.deepcopy(img2)\n","\n","    for l_epiline_coord, r_epiline_coord, left_pt, right_pt in zip(l_epiline_coords, r_epiline_coords,\n","                                                                   np.int32(left_pts), np.int32(right_pts)):\n","        color = tuple(np.random.randint(0, 255, 3).tolist())\n","        img1_copy = cv2.line(img1_copy, tuple(l_epiline_coord[0]), tuple(l_epiline_coord[1]), color, 2)\n","        img1_copy = cv2.circle(img1_copy, (left_pt[0][0], left_pt[0][1]), 7, color, -1)\n","        img2_copy = cv2.line(img2_copy, tuple(r_epiline_coord[0]), tuple(r_epiline_coord[1]), color, 2)\n","        img2_copy = cv2.circle(img2_copy, (right_pt[0][0], right_pt[0][1]), 7, color, -1)\n","\n","    return img1_copy, img2_copy"],"metadata":{"id":"sk7OMsFw4xJN","executionInfo":{"status":"ok","timestamp":1744361531748,"user_tz":-420,"elapsed":56,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["# 12. Epipolar Line Transformation After Rectification\n","\n","After the **Stereo Rectification** process, the images are transformed so that the epipolar lines are aligned and parallel.\n","To adjust the coordinates of the epipolar lines according to the new perspective system, we apply the perspective transformation (Perspective Transformation).\n","Formula:\n","$$\n","\\mathbf{x}_{\\text{warped}} = H \\,\\mathbf{x}\n","$$\n","where \\( H \\) is the homography matrix and \\( \\mathbf{x} \\) is represented in the homography form."],"metadata":{"id":"LsB56fOo5O2Q"}},{"cell_type":"code","source":["def warped_epiline_coords(l_epiline_coords, r_epiline_coords, H1, H2):\n","    l_epiline_coords = np.array(l_epiline_coords, dtype=np.float32)\n","    r_epiline_coords = np.array(r_epiline_coords, dtype=np.float32)\n","\n","    l_epiline_start_coords = l_epiline_coords[:, 0].reshape(-1,1,2)\n","    l_epiline_end_coords = l_epiline_coords[:, 1].reshape(-1,1,2)\n","\n","    r_epiline_start_coords = r_epiline_coords[:, 0].reshape(-1,1,2)\n","    r_epiline_end_coords = r_epiline_coords[:, 1].reshape(-1,1,2)\n","\n","    warped_l_epiline_start_coords = cv2.perspectiveTransform(l_epiline_start_coords, H1).squeeze()\n","    warped_l_epiline_end_coords = cv2.perspectiveTransform(l_epiline_end_coords, H1).squeeze()\n","\n","    warped_r_epiline_start_coords = cv2.perspectiveTransform(r_epiline_start_coords, H2).squeeze()\n","    warped_r_epiline_end_coords = cv2.perspectiveTransform(r_epiline_end_coords, H2).squeeze()\n","\n","    warped_l_epiline_coords = []\n","    warped_r_epiline_coords = []\n","\n","    for start, end in zip(warped_l_epiline_start_coords, warped_l_epiline_end_coords):\n","        start, end = start.astype(int), end.astype(int)\n","        warped_l_epiline_coords.append([start, end])\n","\n","    for start, end in zip(warped_r_epiline_start_coords, warped_r_epiline_end_coords):\n","        start, end = start.astype(int), end.astype(int)\n","        warped_r_epiline_coords.append([start, end])\n","\n","    return warped_l_epiline_coords, warped_r_epiline_coords"],"metadata":{"id":"gJaAz07s5Svq","executionInfo":{"status":"ok","timestamp":1744361532069,"user_tz":-420,"elapsed":47,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":["# 13. Calculate the Sum of Absolute Differences (SAD)\n","\n","### Objective:\n","- Calculate a measure of the difference between two image blocks of the same size by calculating the sum of the absolute values ​​of the differences between the pixels.\n","\n","Formula:\n","$$\n","\\text{SAD} = \\sum_{i,j} \\left| I_1(i,j) - I_2(i,j) \\right|\n","$$\n","\n","The smaller the SAD value, the more similar the image blocks are in terms of brightness values."],"metadata":{"id":"AuFqwSFp5XqD"}},{"cell_type":"code","source":["def sum_of_abs_diff(pixel_vals_1, pixel_vals_2):\n","    if pixel_vals_1.shape != pixel_vals_2.shape:\n","        return -1\n","    return np.sum(abs(pixel_vals_1 - pixel_vals_2))"],"metadata":{"id":"I02Ju6im5ddI","executionInfo":{"status":"ok","timestamp":1744361532366,"user_tz":-420,"elapsed":43,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":["# 14. Block Matching and Disparity Calculation\n","\n","### Objective:\n","- Apply the **Block Matching** method on the rectified image to determine the disparity between the corresponding pixels in the left and right images.\n","\n","### Procedure:\n","1. For each block (window size x window) in the left image at position \\((y,x)\\), we extract this block.\n","\n","2. In the right image, within a horizontal search range (search_range), we match the block by calculating **SAD** for each position.\n","\n","3. The position with the lowest SAD value is selected as the matching position, and the disparity is calculated as the difference between the initial \\(x\\) coordinate and the \\(x\\) of the matching block:\n","\n","$$ \\text{disparity} = \\lvert x_{\\text{match}} - x \\rvert. $$"],"metadata":{"id":"ONvnYO9R5iL8"}},{"cell_type":"code","source":["def compare_blocks(y, x, block_left, right_array, window, search_range):\n","    x_min = max(0, x - search_range)\n","    x_max = min(right_array.shape[1], x + search_range)\n","    first = True\n","    min_sad = None\n","    min_index = None\n","    for xi in range(x_min, x_max):\n","        block_right = right_array[y:y + window, xi: xi + window]\n","        sad = sum_of_abs_diff(block_left, block_right)\n","        if first:\n","            min_sad = sad\n","            min_index = (y, xi)\n","            first = False\n","        else:\n","            if sad < min_sad:\n","                min_sad = sad\n","                min_index = (y, xi)\n","    return min_index"],"metadata":{"id":"s8dZ3a-c5llB","executionInfo":{"status":"ok","timestamp":1744361532684,"user_tz":-420,"elapsed":53,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["#15. Main Code: Integrating All Steps\n","\n","In the final step, we combine all the components built above to perform 3D reconstruction from 2D images. The main steps include:\n","\n","1. **Read the image and detect features:**\n","Use the `feature_detection` function to get the pairs of matching points between two images.\n","\n","2. **Estimate the Fundamental Matrix:**\n","Use RANSAC (the `ransac` function) to estimate the \\(F\\) matrix from the matching points.\n","\n","3. **Calculate the Essential Matrix and decompose:**\n","Use the `essential_matrix` function to calculate \\(E\\) and then use the `decompose_essential_matrix` function to get the probabilities of \\(R\\) and \\(t\\).\n","\n","4. **Triangulation:**\n","Use the `linear_triangulation` function to calculate 3D points from 2D point pairs and projection matrices generated by \\(K\\), \\(R\\) and \\(t\\).\n","\n","5. **Disambiguation:**\n","Based on the number of points with “positive depth”, choose the real \\(R\\) and \\(t\\) configurations with the `disambiguate_camera_pose` function.\n","\n","6. **Stereo Rectification and drawing epipolar lines:**\n","Use the `cv2.stereoRectifyUncalibrated` function to calculate the homography \\(H_1\\) and \\(H_2\\) for the two images, then warp the images to the corrected perspective system and calculate the coordinates, draw the epipolar lines with the `epiline_coordinates` and `draw_epilines` functions.\n","\n","7. **Block matching and calculating Disparity Map:**\n","With the rectified image, apply block matching using the SAD method to calculate the disparity map.\n","\n","8. **Calculate Depth Map:**\n","Use the formula:\n","$$ Z = \\frac{f \\, B}{\\text{disparity} + \\epsilon}, $$\n","where \\( f \\) is the focal length, \\( B \\) is the baseline (distance between the 2 cameras) and \\( \\epsilon \\) is a small constant to avoid dividing by 0."],"metadata":{"id":"jRM63LBT6LFH"}},{"cell_type":"code","source":["img1 = cv2.imread(\"/content/anh1.jpg\")\n","img2 = cv2.imread(\"/content/anh2.jpg\")\n","\n","left_pts, right_pts = feature_detection(img1, img2)\n","\n","F = ransac(left_pts, right_pts, 1500, 0.02)\n","\n","K1 = np.array([[1924, 0, 1284],\n","               [0, 1924, 963],\n","               [0, 0, 1]])\n","K2 = np.array([[1924, 0, 1284],\n","               [0, 1924, 963],\n","               [0, 0, 1]])\n","E = essential_matrix(F, K1, K2)\n","\n","R_set, C_set = decompose_essential_matrix(E)\n","x3D_set = linear_triangulation(R_set, C_set, left_pts, right_pts, K1, K2)\n","R, T = disambiguate_camera_pose(R_set, C_set, x3D_set)\n","\n","print(\"Rotation Matrix R:\")\n","print(R)\n","print(\"Translation Vector T:\")\n","print(T)\n","\n","l_epilines = cv2.computeCorrespondEpilines(right_pts.reshape(-1, 1, 2), 2, F)\n","r_epilines = cv2.computeCorrespondEpilines(left_pts.reshape(-1, 1, 2), 1, F)\n","\n","h_img, w_img = img1.shape[:2]\n","imageSize = (w_img, h_img)\n","\n","distCoeffs1 = np.zeros((5,1))\n","distCoeffs2 = np.zeros((5,1))\n","\n","R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(K1, distCoeffs1,\n","                                                   K2, distCoeffs2,\n","                                                   imageSize, R, T,\n","                                                   flags=0)\n","\n","map1x, map1y = cv2.initUndistortRectifyMap(K1, distCoeffs1, R1, P1, imageSize, cv2.CV_32FC1)\n","map2x, map2y = cv2.initUndistortRectifyMap(K2, distCoeffs2, R2, P2, imageSize, cv2.CV_32FC1)\n","\n","img1_rectified = cv2.remap(img1, map1x, map1y, cv2.INTER_LINEAR)\n","img2_rectified = cv2.remap(img2, map2x, map2y, cv2.INTER_LINEAR)\n","\n","H1 = P1[:, :3] @ np.linalg.inv(K1)\n","H2 = P2[:, :3] @ np.linalg.inv(K2)\n","print(\"Homography H1:\")\n","print(H1)\n","print(\"Homography H2:\")\n","print(H2)\n","\n","l_epiline_coords = epiline_coordinates(l_epilines, img1)\n","r_epiline_coordinates = epiline_coordinates(r_epilines, img2)\n","a, b = draw_epilines(l_epiline_coords, r_epiline_coordinates,\n","                     left_pts.reshape(-1, 1, 2),\n","                     right_pts.reshape(-1, 1, 2),\n","                     img1, img2)\n","out = np.hstack((a, b))\n","\n","img1_rectified_points = cv2.warpPerspective(img1, H1, (w_img, h_img))\n","img2_rectified_points = cv2.warpPerspective(img2, H2, (w_img, h_img))\n","out_rectified = np.hstack((img1_rectified_points, img2_rectified_points))\n","\n","left_dst_pts = cv2.perspectiveTransform(left_pts.reshape(-1, 1, 2), H1).squeeze()\n","right_dst_pts = cv2.perspectiveTransform(right_pts.reshape(-1, 1, 2), H2).squeeze()\n","\n","warped_l_epiline_coords, warped_r_epiline_coords = warped_epiline_coords(l_epiline_coords,\n","                                                                         r_epiline_coordinates,\n","                                                                         H1, H2)\n","a_rect, b_rect = draw_epilines(warped_l_epiline_coords, warped_r_epiline_coords,\n","                               left_dst_pts.reshape(-1, 1, 2),\n","                               right_dst_pts.reshape(-1, 1, 2),\n","                               img1_rectified_points, img2_rectified_points)\n","out_epilines = np.hstack((a_rect, b_rect))\n","cv2.imwrite('rectified_epilines.png', out_epilines)\n","\n","a_gray = cv2.cvtColor(img1_rectified, cv2.COLOR_BGR2GRAY)\n","b_gray = cv2.cvtColor(img2_rectified, cv2.COLOR_BGR2GRAY)\n","a_gray = a_gray.astype(float)\n","b_gray = b_gray.astype(float)\n","h_gray, w_gray = a_gray.shape\n","\n","disparity_map = np.zeros((h_gray, w_gray))\n","window = 7\n","search_range = 56\n","\n","for y in tqdm(range(window, h_gray - window)):\n","    for x in range(window, w_gray - window):\n","        block_left = a_gray[y:y + window, x:x + window]\n","        min_index = compare_blocks(y, x, block_left, b_gray, window, search_range)\n","        disparity_map[y, x] = abs(min_index[1] - x)\n","\n","disparity = np.uint8(disparity_map * 255 / np.max(disparity_map))\n","heatmap = cv2.applyColorMap(disparity, cv2.COLORMAP_JET)\n","\n","f = K1[0, 0]\n","baseline = 88.39\n","depth = (baseline * f) / (disparity + 1e-15)\n","depth[depth > 30000] = 30000\n","depth_map = np.uint8(depth * 255 / np.max(depth))\n","heatmap_depth = cv2.applyColorMap(depth_map, cv2.COLORMAP_JET)\n","cv2.imwrite('depth_map.png', heatmap_depth)"],"metadata":{"id":"M1dsaFCI6RQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744365828755,"user_tz":-420,"elapsed":4295751,"user":{"displayName":"Mai Anh Trường","userId":"15182437648677258602"}},"outputId":"b8a68c23-8c04-4630-da3b-03556dea0368"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["Rotation Matrix R:\n","[[ 1.          0.          0.        ]\n"," [ 0.          0.99987731  0.01566429]\n"," [ 0.         -0.01566429  0.99987731]]\n","Translation Vector T:\n","[1. 0. 0.]\n","Homography H1:\n","[[1.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 1.00000000e+00 6.10351562e-05]\n"," [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n","Homography H2:\n","[[1.00000000e+00 0.00000000e+00 3.05175781e-05]\n"," [0.00000000e+00 1.00000000e+00 6.10351562e-05]\n"," [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1912/1912 [1:11:29<00:00,  2.24s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":98}]}]}